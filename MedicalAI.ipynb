{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMX7SVa5LCfCjWMzSjdT7Ce"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcxym84QghJX"
      },
      "outputs": [],
      "source": [
        "#Step1 First select T4 GPU in google collab\n",
        "#Step2 Create Hugging Face Token and provide to google collab as Key/Value Secret\n",
        "print(\"MedicalAI\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step3: Install required dependencies\n",
        "!pip install unsloth # install unsloth\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xe8_rlGei1e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step4: Import necessary libraries\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "from huggingface_hub import login\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import wandb"
      ],
      "metadata": {
        "id": "Fh71PAJbnXGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step5: Validate HF token\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('hf_medical_AI')\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "8NZfdtPlnu61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step6: Create wandb api key in wandb application, Weights & Biases' tools make it easy for you to quickly\n",
        "# track experiments, visualize results, spot regressions, and more. Simply put, Weights & Biases enables\n",
        "# you to build better models faster and easily share findings with colleagues.\n",
        "wandb_medical_AI = userdata.get('wandb_medical_AI')\n",
        "print(wandb_medical_AI)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "__-TLM7tqiBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step7: Setup pretrained DeepSeek-R1\n",
        "\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "max_sequence_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_sequence_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = hf_token\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2knfp56LtFQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step8: Setup system prompt\n",
        "prompt_style = \"\"\"\n",
        "Below is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.\n",
        "\n",
        "Before crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.\n",
        "\n",
        "### Task:\n",
        "You are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.\n",
        "\n",
        "### Query:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GKA1bCepuf1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step9: Run Inference on the model\n",
        "\n",
        "# Define a test question\n",
        "question = \"\"\"A 75-year-old man with a long history of involuntary urine loss during activities like coughing or\n",
        "              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
        "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response\n",
        "outputs = model.generate (\n",
        "    input_ids = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask,\n",
        "    max_new_tokens = 1200,\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "# Decode the response tokens back to text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "\n",
        "print(response[0].split(\"### Answer:\")[1])"
      ],
      "metadata": {
        "id": "yuuCtlEVux1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step10: Setup fine-tuning\n",
        "\n",
        "# Load Dataset from hugging face\n",
        "medical_dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split = \"train[:500]\", trust_remote_code = True)\n",
        ""
      ],
      "metadata": {
        "collapsed": true,
        "id": "9t3mkTqmvuLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which tells the model when to stop generating text during training\n",
        "EOS_TOKEN"
      ],
      "metadata": {
        "id": "kzl86ZE9xAWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Finetuning\n",
        "# Updated training prompt style to add  tag\n",
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0bn-JfwlxykU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for fine-tuning\n",
        "\n",
        "def preprocess_input_data(examples):\n",
        "  inputs = examples[\"Question\"]\n",
        "  cots = examples[\"Complex_CoT\"]\n",
        "  outputs = examples[\"Response\"]\n",
        "\n",
        "  texts = []\n",
        "\n",
        "  for input, cot, output in zip(inputs, cots, outputs):\n",
        "    text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
        "    texts.append(text)\n",
        "\n",
        "  return {\n",
        "      \"texts\" : texts,\n",
        "  }"
      ],
      "metadata": {
        "id": "BAP4S69wxV_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_dataset = medical_dataset.map(preprocess_input_data, batched = True)"
      ],
      "metadata": {
        "id": "rFrHd0AWynv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_dataset[\"texts\"][0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Zk9CGo77y3L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step11: Setup/Apply LoRA finetuning to the model\n",
        "\n",
        "model_lora = FastLanguageModel.get_peft_model(\n",
        "    model = model,\n",
        "    r = 16,\n",
        "    target_modules = [\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3047,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None\n",
        ")"
      ],
      "metadata": {
        "id": "YyXLrjVbzTqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this before creating the trainer\n",
        "if hasattr(model, '_unwrapped_old_generate'):\n",
        "    del model._unwrapped_old_generate"
      ],
      "metadata": {
        "id": "2CDrKqbm0ccy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model_lora,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = finetune_dataset,\n",
        "    dataset_text_field = \"texts\",\n",
        "    max_seq_length = max_sequence_length,\n",
        "    dataset_num_proc = 1,\n",
        "\n",
        "    # Define training args\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        num_train_epochs = 1,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "aUPMVSLe0osm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup WANDB\n",
        "from google.colab import userdata\n",
        "wnb_token = userdata.get(\"wandb_medical_AI\")\n",
        "# Login to WnB\n",
        "wandb.login(key=wnb_token) # import wandb\n",
        "run = wandb.init(\n",
        "    project='Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CaEvpl8k1MxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the fine-tuning process\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PSml2vkv1vtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3xsVn3hX3NWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step12: Testing after fine-tuning\n",
        "question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing\n",
        "              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,\n",
        "              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model_lora)\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response\n",
        "outputs = model_lora.generate (\n",
        "    input_ids = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask,\n",
        "    max_new_tokens = 1200,\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "# Decode the response tokens back to text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "print(response[0].split(\"### Answer:\")[1])"
      ],
      "metadata": {
        "id": "lTtAMuN33Y7U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}